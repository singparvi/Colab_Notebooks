{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"LS_DS_222_assignment.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"-Q_ju3fn2xwB"},"source":["Lambda School Data Science\n","\n","*Unit 2, Sprint 2, Module 2*\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"7IXUfiQ2UKj6"},"source":["# Random Forests\n","\n","## Assignment\n","- [ ] Read [“Adopting a Hypothesis-Driven Workflow”](http://archive.is/Nu3EI), a blog post by a Lambda DS student about the Tanzania Waterpumps challenge.\n","- [ ] Continue to participate in our Kaggle challenge.\n","- [ ] Define a function to wrangle train, validate, and test sets in the same way. Clean outliers and engineer features.\n","- [ ] Try Ordinal Encoding.\n","- [ ] Try a Random Forest Classifier.\n","- [ ] Submit your predictions to our Kaggle competition. (Go to our Kaggle InClass competition webpage. Use the blue **Submit Predictions** button to upload your CSV file. Or you can use the Kaggle API to submit your predictions.)\n","- [ ] Commit your notebook to your fork of the GitHub repo.\n","\n","## Stretch Goals\n","\n","### Doing\n","- [ ] Add your own stretch goal(s) !\n","- [ ] Do more exploratory data analysis, data cleaning, feature engineering, and feature selection.\n","- [ ] Try other [categorical encodings](https://contrib.scikit-learn.org/category_encoders/).\n","- [ ] Get and plot your feature importances.\n","- [ ] Make visualizations and share on Slack.\n","\n","### Reading\n","\n","Top recommendations in _**bold italic:**_\n","\n","#### Decision Trees\n","- A Visual Introduction to Machine Learning, [Part 1: A Decision Tree](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/),  and _**[Part 2: Bias and Variance](http://www.r2d3.us/visual-intro-to-machine-learning-part-2/)**_\n","- [Decision Trees: Advantages & Disadvantages](https://christophm.github.io/interpretable-ml-book/tree.html#advantages-2)\n","- [How a Russian mathematician constructed a decision tree — by hand — to solve a medical problem](http://fastml.com/how-a-russian-mathematician-constructed-a-decision-tree-by-hand-to-solve-a-medical-problem/)\n","- [How decision trees work](https://brohrer.github.io/how_decision_trees_work.html)\n","- [Let’s Write a Decision Tree Classifier from Scratch](https://www.youtube.com/watch?v=LDRbO9a6XPU)\n","\n","#### Random Forests\n","- [_An Introduction to Statistical Learning_](http://www-bcf.usc.edu/~gareth/ISL/), Chapter 8: Tree-Based Methods\n","- [Coloring with Random Forests](http://structuringtheunstructured.blogspot.com/2017/11/coloring-with-random-forests.html)\n","- _**[Random Forests for Complete Beginners: The definitive guide to Random Forests and Decision Trees](https://victorzhou.com/blog/intro-to-random-forests/)**_\n","\n","#### Categorical encoding for trees\n","- [Are categorical variables getting lost in your random forests?](https://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/)\n","- [Beyond One-Hot: An Exploration of Categorical Variables](http://www.willmcginnis.com/2015/11/29/beyond-one-hot-an-exploration-of-categorical-variables/)\n","- _**[Categorical Features and Encoding in Decision Trees](https://medium.com/data-design/visiting-categorical-features-and-encoding-in-decision-trees-53400fa65931)**_\n","- _**[Coursera — How to Win a Data Science Competition: Learn from Top Kagglers — Concept of mean encoding](https://www.coursera.org/lecture/competitive-data-science/concept-of-mean-encoding-b5Gxv)**_\n","- [Mean (likelihood) encodings: a comprehensive study](https://www.kaggle.com/vprokopev/mean-likelihood-encodings-a-comprehensive-study)\n","- [The Mechanics of Machine Learning, Chapter 6: Categorically Speaking](https://mlbook.explained.ai/catvars.html)\n","\n","#### Imposter Syndrome\n","- [Effort Shock and Reward Shock (How The Karate Kid Ruined The Modern World)](http://www.tempobook.com/2014/07/09/effort-shock-and-reward-shock/)\n","- [How to manage impostor syndrome in data science](https://towardsdatascience.com/how-to-manage-impostor-syndrome-in-data-science-ad814809f068)\n","- [\"I am not a real data scientist\"](https://brohrer.github.io/imposter_syndrome.html)\n","- _**[Imposter Syndrome in Data Science](https://caitlinhudon.com/2018/01/19/imposter-syndrome-in-data-science/)**_\n","\n","\n","### More Categorical Encodings\n","\n","**1.** The article **[Categorical Features and Encoding in Decision Trees](https://medium.com/data-design/visiting-categorical-features-and-encoding-in-decision-trees-53400fa65931)** mentions 4 encodings:\n","\n","- **\"Categorical Encoding\":** This means using the raw categorical values as-is, not encoded. Scikit-learn doesn't support this, but some tree algorithm implementations do. For example, [Catboost](https://catboost.ai/), or R's [rpart](https://cran.r-project.org/web/packages/rpart/index.html) package.\n","- **Numeric Encoding:** Synonymous with Label Encoding, or \"Ordinal\" Encoding with random order. We can use [category_encoders.OrdinalEncoder](https://contrib.scikit-learn.org/category_encoders/ordinal.html).\n","- **One-Hot Encoding:** We can use [category_encoders.OneHotEncoder](https://contrib.scikit-learn.org/category_encoders/onehot.html).\n","- **Binary Encoding:** We can use [category_encoders.BinaryEncoder](https://contrib.scikit-learn.org/category_encoders/binary.html).\n","\n","\n","**2.** The short video \n","**[Coursera — How to Win a Data Science Competition: Learn from Top Kagglers — Concept of mean encoding](https://www.coursera.org/lecture/competitive-data-science/concept-of-mean-encoding-b5Gxv)** introduces an interesting idea: use both X _and_ y to encode categoricals.\n","\n","Category Encoders has multiple implementations of this general concept:\n","\n","- [CatBoost Encoder](https://contrib.scikit-learn.org/category_encoders/catboost.html)\n","- [Generalized Linear Mixed Model Encoder](https://contrib.scikit-learn.org/category_encoders/glmm.html)\n","- [James-Stein Encoder](https://contrib.scikit-learn.org/category_encoders/jamesstein.html)\n","- [Leave One Out](https://contrib.scikit-learn.org/category_encoders/leaveoneout.html)\n","- [M-estimate](https://contrib.scikit-learn.org/category_encoders/mestimate.html)\n","- [Target Encoder](https://contrib.scikit-learn.org/category_encoders/targetencoder.html)\n","- [Weight of Evidence](https://contrib.scikit-learn.org/category_encoders/woe.html)\n","\n","Category Encoder's mean encoding implementations work for regression problems or binary classification problems. \n","\n","For multi-class classification problems, you will need to temporarily reformulate it as binary classification. For example:\n","\n","```python\n","encoder = ce.TargetEncoder(min_samples_leaf=..., smoothing=...) # Both parameters > 1 to avoid overfitting\n","X_train_encoded = encoder.fit_transform(X_train, y_train=='functional')\n","X_val_encoded = encoder.transform(X_train, y_val=='functional')\n","```\n","\n","For this reason, mean encoding won't work well within pipelines for multi-class classification problems.\n","\n","**3.** The **[dirty_cat](https://dirty-cat.github.io/stable/)** library has a Target Encoder implementation that works with multi-class classification.\n","\n","```python\n"," dirty_cat.TargetEncoder(clf_type='multiclass-clf')\n","```\n","It also implements an interesting idea called [\"Similarity Encoder\" for dirty categories](https://www.slideshare.net/GaelVaroquaux/machine-learning-on-non-curated-data-154905090).\n","\n","However, it seems like dirty_cat doesn't handle missing values or unknown categories as well as category_encoders does. And you may need to use it with one column at a time, instead of with your whole dataframe.\n","\n","**4. [Embeddings](https://www.kaggle.com/colinmorris/embedding-layers)** can work well with sparse / high cardinality categoricals.\n","\n","_**I hope it’s not too frustrating or confusing that there’s not one “canonical” way to encode categoricals. It’s an active area of research and experimentation — maybe you can make your own contributions!**_"]},{"cell_type":"markdown","metadata":{"id":"9jloSo-y2xwF"},"source":["### Setup\n","\n","You can work locally (follow the [local setup instructions](https://lambdaschool.github.io/ds/unit2/local/)) or on Colab (run the code cell below)."]},{"cell_type":"code","metadata":{"id":"o9eSnDYhUGD7"},"source":["%%capture\n","import sys\n","\n","# If you're on Colab:\n","if 'google.colab' in sys.modules:\n","    DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Kaggle-Challenge/master/data/'\n","    !pip install category_encoders==2.*\n","\n","# If you're working locally:\n","else:\n","    DATA_PATH = '../data/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QJBD4ruICm1m"},"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","train = pd.merge(pd.read_csv(DATA_PATH+'waterpumps/train_features.csv'), \n","                 pd.read_csv(DATA_PATH+'waterpumps/train_labels.csv'))\n","test = pd.read_csv(DATA_PATH+'waterpumps/test_features.csv')\n","sample_submission = pd.read_csv(DATA_PATH+'waterpumps/sample_submission.csv')\n","\n","train.shape, test.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ruWLix7-2xwG"},"source":[""],"execution_count":null,"outputs":[]}]}